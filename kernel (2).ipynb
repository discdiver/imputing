{
  "cells": [
    {
      "metadata": {
        "_uuid": "201ed9ef0270604a23cd6c642162ea80845d6195"
      },
      "cell_type": "markdown",
      "source": "# Imputing Values of Machine Learning - Data Types not Dtypes\n## By Jeff Hale"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "Machine learning algorithms in sklearn require that there be no missing data. Some machine learning algorithms that can be implemented outside sklearn can automatically impute missing data. In this post we're going to assume you want to learn all the sklean goodness such as pipelines, so we're going to to make sure our pandas data frames don't have any missing data.\n\nThere are a variety of ways to deal with missing data in machine learning. \n\nOne option is to delete observations or variables with any missing data. If a very small proporition of observations or variables have missing data dropping them might not have much of an effect on a model's performance. But generally we don't want to throw away data that isn't erroneous - it might include some bit of meaning that could theoretically help a machine learning model perform better. \n\nFor example, in the popular [Ames Housing Dataset ](http://http://ww2.amstat.org/publications/jse/v19n3/decock.pdf) very few of the samples in the dataset have a value for the Pool Quality variable, yet we might expect that property with a pool in good shape is more valuable than a similar property with a pool in poor shape. A model might use that information to more accurately predict the sale price of the property.  Drop that column from the dataframe and that opportunity is lost.\n\nLikewise you don't want to drop observations just because they don't have a value for the pool quality variable because you wouldn't have many observations left :) \n\nOf couse the true test for the value of imputing missing data for  any individual data set is whether a model can use the missing data to perform better or not. The goal of the current project  is to develop some processes  and code snippets as guidelines or starting points that will be helpful for imputing data. We want a process to impute data in a manner that is quick and useful for machine learning pipelines. \n\nI also hope to expose readers to a wider range of options than you might have known were possible to help you deal with missing data.\n\nSo how should we fill the missing values? Like lots of things in Data Science there are a slew of different options. \n\nFor the absolute best results you should try several approaches. But because there is a time and effort tradeoff and there are so many aspects of a problem to look at, we want a quick way to handle missing values that does a good enough job.\n\nImputer options for interval features:\n* Mean\n* Median \n* Mode - probably only use if really ordinal or nominal data.\n\nOptions for string data type\n*If one hot encoding categorical data, we can also make a one hot encoded column for missing data for a feature.\n\nSpecial options for time series data:\n*Fill forward\n*Fill backward\n*Average of backward and forward\n*Any of the above options combined with a seasonality factor\n\nOptions for any type of data:\n*Use a model such as KNN to determine how similar the data is.\n*Use multiple imputation that solves some problems and is more thorough than any single imputation. MICE is available in Fancy Imputer (Multiple Imputation by Chained Equations)  *from fancyimpute import MICE*\n\nThe important thing is the type of data (interval, ordinal, or nominal) - not the original dtype. Everything is going to be made into integer or float data for use in the machine learning model.\n\nThen after imputation, there is the additional choice:\n*Create a column for each predictor variable to note whether the final value was imputed for that elent or not. This is suggested by Dan Becker in Kaggle's guide [Handling Missing Values](https://www.kaggle.com/dansbecker/handling-missing-values). \n\nHere's a [great post](https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4) on imputing values by Alvira Swalin. The code is in R. \n\nHere's more info on [MICE:](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/).\n\nFancyimpute also provides implementation of basic imputation, KNN, and other methods in addition to MICE [fancyimpute package](https://github.com/iskandr/fancyimpute).\n\nUsing MICE is generally better because it creates some randomness."
    },
    {
      "metadata": {
        "_uuid": "964747c1f1e5e3c0503ff63341bade1be957943d"
      },
      "cell_type": "markdown",
      "source": "## Measurement scales\n\nWe can't talk about imputing without talking about the measurement scales of variables.\n\nIt seems that machine learning guides and tutorials often confuse terminology around measurement scales. (https://www.mymarketresearchmethods.com/types-of-data-nominal-ordinal-interval-ratio/)\n\nInterval and ratio numeric data is the nice easy stuff to use in machine learning. I group ratio data here as it doesn't really matter for machine learning whether it's true interval or ratio data. \n\nOrdinal data might come to you as an integer data type or as string (object) data type. How you treat it should not depend on whether it comes to you as a number or a string. What matters is whether the ordinal data is close enough to interval data to treat it as interval data. Social scientists make this assumption all the time with likert scales (e.g. On a scale from 1 to 7, 1 being extremely unlikely, 4 being neither likely nor unlikely and 7 being exteremley likely, how likely are you to recommend this movie to a friend?). Here the difference between 3 and 4 and the difference between 6 and 7 can be reasonably assumed to be similar.\n\nIf a column is ordinal but can't reasonably be assumed to be approximately interval, then it might make sense to make a new binary variable and split the data with sklearn's Binarizer so that high values receive a 1 and low values receive a 0.  You could also bin the values into a few numeric  categories and try treating those as interval data.\n\nAlternatively you can treat ordinal data as nominal data and do what you always do with nominal data - one hot encode it. This gets to be a bit cumbersome if there are lots of values for a variable.\n\nNominal data might also come to you in numeric form, but correspond to categories that have no numeric relationship to each other. That's fine, you just saved a step.\n\nYou can try PCA to select a limited number of features to avoid the curse of dimensionality on small data sets with large numbers of nominal variables you've one hot encoded.\n\ncheck out categorical encoder and will mcginness blog again.\n\nOr in some data sets, such as the Ames set, the categorical data might not have much new data not in other features, so it can be dropped."
    },
    {
      "metadata": {
        "_uuid": "6294a8f33127755d312054675195a0c200eab38c"
      },
      "cell_type": "markdown",
      "source": "Here's a working process (also a work-in-process) for dealing with missing values. \n\n1. Your first task is to figure out which data scale each column is. Write out all your variables with definitions  and if discrete, their possible values in a spreadsheet. Note their dtype and what type of data measurement scale they are. Write out the variables with definitions and thoughts in a google sheet. [Here's mine for the Ames Housing Dataset](https://docs.google.com/spreadsheets/d/106ZP2r97yRkkTbBqV9oEt00XNnjomhj3BvIaCNaeWlk/edit?usp=sharing). This idea came from [ Pedro Marcelino's popular Kernel](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python). This sheet can be very helpful when you're seeking to understand your data and when you looking to create new features. \n2. Once you've decided what measurement scale each variable is, turn it into numbers.\n3. Use MICE to impute the missing values.\n4. Create columns of values denoting whether a value was imputed.\n4. One hot encode nominal data.\n5. Bin/binarize as needed.\n6. Proceed with preprocessing to evaluate outliers, scale and transform data, feature engineer, etc.\n\n\nHave improvements? Please share them in the comments."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.base import TransformerMixin\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "94605c87a635ec84167b392e70a8f8cd73ce94b8"
      },
      "cell_type": "markdown",
      "source": "Basic imputer that imputes differently depending upon datatype. Often will need to tweak depending upon time series, ordinal, or other data characteristics. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8043a1149913159416c1a9d65e1e7639620a4210",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "class DataFrameImputer(TransformerMixin):\n\n    def __init__(self):\n        \"\"\"Impute missing values.\n\n        Columns of dtype object are imputed with the mode.\n\n        Columns of numberical dtypes are imputed with mean of column.\n        \n        need a way to denote ordinal data more easily in pandas\n\n        \"\"\"\n    def fit(self, X, y=None):\n\n        self.fill = pd.Series([X[c].value_counts().index[0]\n            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n            index=X.columns)\n        # how treat boolean data?\n\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(self.fill)\n    \n    # make columns to track whether a value was imputed?\n    # make a column for one hot encode that notes the data was missing?\n\n\n#train_data = DataFrameImputer().fit_transform(train_data)\n#test_data = DataFrameImputer().fit_transform(test_data)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}