{
  "cells": [
    {
      "metadata": {
        "_uuid": "201ed9ef0270604a23cd6c642162ea80845d6195"
      },
      "cell_type": "markdown",
      "source": "## Imputing Values for Machine Learning\n## By Jeff Hale"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "Machine learning algorithms in sklearn require that there be no missing data. Some machine learning algorithms that can be implemented outside sklearn can automatically impute missing data. In this post we're going to assume you want to learn all the sklean goodness such as pipelines, so we're going to to make sure our pandas data frames don't have any missing data.\n\nThere are a variety of ways to deal with missing data in machine learning. \n\nOne option is to delete observations or variables with any missing data. If a very small proporition of observations or variables have missing data dropping them might not have much of an effect on a model's performance. But generally we don't want to throw away data that isn't erroneous - it might include some bit of meaning that could theoretically help a machine learning model perform better. \n\nFor example, in the popular [Ames Housing Dataset ](http://http://ww2.amstat.org/publications/jse/v19n3/decock.pdf) very few of the samples in the dataset have a value for the Pool Quality variable, yet we might expect that property with a pool in good shape is more valuable than a similar property with a pool in poor shape. A model might use that information to more accurately predict the sale price of the property.  Drop that column from the dataframe and that opportunity is lost.\n\nLikewise you don't want to drop observations just because they don't have a value for the pool quality variable because you wouldn't have many observations left :) \n\nOf couse the true test for the value of imputing missing data for  any individual data set is whether a model can use the missing data to perform better or not. The goal of the current project  is to develop some processes  and code snippets as guidelines or starting points that will be helpful for imputing data. We want a process to impute data in a manner that is quick and useful for machine learning pipelines. \n\nI also hope to expose readers to a wider range of options than you might have known were possible to help you deal with missing data.\n\nSo how should we fill the missing values? Like lots of things in Data Science there are a slew of different options. \n\nFor the absolute best results you should try several approaches. But because there is a time and effort tradeoff and there are so many aspects of a problem to look at, we want a quick way to handle missing values that does a good enough job.\n\nImputer options for interval features:\n* Mean\n* Median \n* Mode - probably only use if really ordinal or nominal data.\n\nOptions for string data type\n*If one hot encoding categorical data, we can also make a one hot encoded column for missing data for a feature.\n\nSpecial options for time series data:\n*Fill forward\n*Fill backward\n*Average of backward and forward\n*Any of the above options combined with a seasonality factor\n\nOptions for any type of data:\n*Use a model such as KNN to determine how similar the data is.\n*Use multiple imputation that solves some problems and is more thorough than any single imputation. MICE is available in Fancy Imputer (Multiple Imputation by Chained Equations)  *from fancyimpute import MICE*\n\nThe important thing is the type of data (interval, ordinal, or nominal) - not the original dtype. Everything is going to be made into integer or float data for use in the machine learning model.\n\nThen after imputation, there is the additional choice:\n*Create a column for each predictor variable to note whether the final value was imputed for that elent or not. This is suggested by Dan Becker in Kaggle's guide [Handling Missing Values](https://www.kaggle.com/dansbecker/handling-missing-values). \n\nHere's a [great post](https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4) on imputing values by Alvira Swalin. The code is in R. \n\nHere's more info on [MICE:](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/).\n\nFancyimpute also provides implementation of basic imputation, KNN, and other methods in addition to MICE [fancyimpute package](https://github.com/iskandr/fancyimpute).\n\nUsing MICE is generally better because it creates some randomness."
    },
    {
      "metadata": {
        "_uuid": "964747c1f1e5e3c0503ff63341bade1be957943d"
      },
      "cell_type": "markdown",
      "source": "## Measurement scales\n\nTo talk about imputing we need to talk about the measurement scales of variables. See the discussion in my previous Kaggle kernel [here](https://www.kaggle.com/discdiver/measurement-scales-for-machine-learning/ )."
    },
    {
      "metadata": {
        "_uuid": "6294a8f33127755d312054675195a0c200eab38c"
      },
      "cell_type": "markdown",
      "source": "Here's a working process (also a work-in-process) for dealing with missing values. \n\n1. Your first task is to figure out which data scale each column is. Write out all your variables with definitions  and if discrete, their possible values in a spreadsheet. Note their dtype and what type of data measurement scale they are. Write out the variables with definitions and thoughts in a google sheet. [Here's mine for the Ames Housing Dataset](https://docs.google.com/spreadsheets/d/106ZP2r97yRkkTbBqV9oEt00XNnjomhj3BvIaCNaeWlk/edit?usp=sharing). This idea came from [ Pedro Marcelino's popular Kernel](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python). This sheet can be very helpful when you're seeking to understand your data and when you looking to create new features. \n2. Once you've decided what measurement scale each variable is, turn the values into numbers.\n3. Use MICE to impute the missing values.\n4. Create columns of values denoting whether a value was imputed.\n4. One hot encode nominal and ordinal data or ideally, try several encoding schemes with [Category Encoders](http://contrib.scikit-learn.org/categorical-encoding/). If you haven't imputed your missing values yet, Category Encoders will make a new column to indicate whether a variable\n5. Bin/binarize as needed.\n6. Proceed with preprocessing to evaluate outliers, scale and transform data, feature engineer, etc.\n\n\nHave improvements? Please share them in the comments."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.base import TransformerMixin\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "94605c87a635ec84167b392e70a8f8cd73ce94b8"
      },
      "cell_type": "markdown",
      "source": "Basic imputer that imputes differently depending upon datatype. Often will need to tweak depending upon time series, ordinal, or other data characteristics. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8043a1149913159416c1a9d65e1e7639620a4210",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "class DataFrameImputer(TransformerMixin):\n\n    def __init__(self):\n        \"\"\"Impute missing values.\n\n        Columns of dtype object are imputed with the mode.\n\n        Columns of numberical dtypes are imputed with mean of column.\n        \n        need a way to denote ordinal data more easily in pandas\n\n        \"\"\"\n    def fit(self, X, y=None):\n\n        self.fill = pd.Series([X[c].value_counts().index[0]\n            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n            index=X.columns)\n        # how treat boolean data?\n\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(self.fill)\n    \n    # make columns to track whether a value was imputed?\n    # make a column for one hot encode that notes the data was missing?\n\n\n#train_data = DataFrameImputer().fit_transform(train_data)\n#test_data = DataFrameImputer().fit_transform(test_data)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}